House Price Prediction Project (Kaggle)
Competition: House Prices - Advanced Regression Techniques
üèÜ Final Result: Rank 519 / 5,277 (Top 10%) | Final RMSLE Score: 0.12316

This project is an end-to-end solution for Kaggle's popular House Prices competition. The goal is to predict the final sale price of homes in Ames, Iowa, using 79 explanatory variables describing (almost) every aspect of residential homes.

The solution is built with an Object-Oriented Programming (OOP) structure that cleanly separates the pipeline's responsibilities: data loading, analysis (EDA), cleaning, preprocessing, and modeling.

üõ†Ô∏è Technologies & Libraries
Python 3.x

Pandas & NumPy: For data loading, processing, and manipulation.

Matplotlib & Seaborn: For exploratory data analysis (EDA) and visualization.

Scikit-learn: Used for:

Preprocessing (StandardScaler).

Model Validation (KFold, cross_val_score).

Optimization (GridSearchCV).

Linear Models (Ridge, Lasso).

XGBoost: For building a powerful Gradient Boosting model.

Kaggle API: For automated data loading.

üìÇ Project Structure
The code is organized into modular classes to increase readability, maintainability, and prevent data leakage.

main.py: The main script that orchestrates the entire pipeline execution.

DataHandler.py: Contains 3 classes for handling data:

ImportData: Downloads data from Kaggle and creates a dictionary of DataFrames.

DataInfo: Contains functions for exploratory data analysis (EDA) (currently commented out in main).

DataCleaning: Handles all data cleaning steps (filling missing values, removing outliers and noise).

MlModel.py: A class containing all modeling logic:

Training and evaluating baseline models.

Optimization using GridSearchCV.

Building the final hybrid model.

housing_data/: (Generated directory) Contains the raw CSV files from the competition.

submission.csv: (Output file) The final submission file generated by main.py.

üöÄ The Pipeline Process
The project follows a structured ML methodology, divided into 4 main stages:

1. Data Cleaning
Handling Missing Values: Filling missing values based on domain logic (e.g., NaN in PoolQC means "No Pool" and was filled with "None"). LotFrontage was imputed using the median value of its Neighborhood.

Handling Outliers: Identified and removed 2-3 extreme outliers from the training set (GrLivArea > 4000) that were harming the model's performance.

Removing Noise: Removed columns with no variance or contribution (Utilities, Street, Id).

2. Preprocessing & Feature Engineering
Feature Engineering: Created new, powerful features like TotalSF (total square footage), TotalBathrooms (total bathrooms), and HouseAge.

Handling Skewness: Applied a log transformation (np.log1p) to the target variable (SalePrice) and other skewed numerical features.

Encoding:

Ordinal: Converted quality-based features (like ExterQual) into ordered numerical values (0-5).

Nominal: Used pd.get_dummies on categorical features (like Neighborhood). Data leakage was prevented by concatenating train and test sets before encoding and splitting them back after.

Standardization:

Used StandardScaler on all final numerical features.

Data leakage was prevented by fitting the scaler only on the training data (X_train) and then transforming both train and test sets.

3. Modeling & Optimization
Baseline Check: Ran several models (Ridge, Lasso, XGBoost) using KFold and cross_val_score to measure baseline RMSE.

Identifying Winners: Linear models (especially Ridge) showed very strong performance (RMSE of ~0.113), indicating the success of the feature engineering and preprocessing steps.

Optimization: Used GridSearchCV to fine-tune the hyperparameters of both Ridge and XGBoost to find their most robust versions.

4. Hybrid Model (Ensemble)
To achieve a competitive score, the final model is a hybrid ensemble that combines the strengths of both model types:

A tuned Ridge model (excellent at capturing linear trends).

A tuned XGBoost model (excellent at capturing complex, non-linear patterns).

The final prediction is a weighted average: 75% weight to the Ridge model and 25% weight to the XGBoost model. This blend cancels out mutual errors and produces a more stable and accurate prediction, leading to the final score.

üèÅ How to Run This Project
Clone the repository:

Bash

git clone https://github.com/YourUsername/YourRepoName.git
cd YourRepoName
Install the required libraries (a requirements.txt file is recommended):

Bash

pip install pandas numpy scikit-learn xgboost kaggle matplotlib seaborn
Ensure you have a Kaggle API key. Download your kaggle.json file from your account and place it in:

~/.kaggle/ (on Linux/Mac)

C:\Users\<YourUsername>\.kaggle\ (on Windows)

Run the main pipeline:

Bash

python main.py
The script will download the data, process it, train the models, and create a submission.csv file in the root directory, ready for submission to Kaggle.
